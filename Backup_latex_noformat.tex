\documentclass[12pt]{report}

\usepackage[a4paper, margin=1in]{geometry}

\usepackage[english]{babel}
\usepackage{amsmath} % Add this in the preamble
\usepackage{amssymb}
\usepackage{enumitem}

\title{Tuo Titolo di Laurea}
\author{Tuo Nome}
\date{\today} % O la data desiderata
\newcommand{\university}{Nome dell'Università}
\newcommand{\faculty}{Nome della Facoltà}


\begin{document}
	\maketitle
	
	\begin{abstract}
		Il tuo abstract qui.
	\end{abstract}
	
	\tableofcontents
	
	\pagestyle{headings}
	
	
	\newpage
	\section*{Selected Notation}
	\addcontentsline{toc}{section}{Selected Notation}
	
	\begin{itemize}[label=\textbullet]
		\item we denote an Hermitian operator as $A^H$ instead of $A^*$ in order to distinguish such notation with the one of the conjugate matrix $A^*$.
		
		\item 
		
		\item 
		
	\end{itemize}
	
	
	\chapter{Toric codes}
	% Altre sezioni
	
	
	\begin{minipage}{1\textwidth}
		
		\textbf{Definition 1.1.} (Torus) A torus is a bidimensional square lattice with periodic boundary conditions.\newline 
		
		Let G be a graph with vertices V and edges E, where V is a set of points representing lattice sites, and E is a set of connections between these points. The lattice is bidimensional, meaning each lattice site has two neighbors in the horizontal and vertical directions.\newline
		
		The square lattice can be represented as a regular grid of lattice sites, typically arranged in rows and columns. Mathematically, it can be defined as a pair (V, E) such that:\newline
		
		1. V is a set of points $(x, y)$, where $x$ and $y$ are integers representing the coordinates of lattice sites, and $x, y \in \mathbb{Z} $.\newline
		
		2. E is a set of edges connecting neighboring lattice sites. If $(x1, y1)$ and $(x2, y2)$ are two lattice sites, there is an edge $(x1, y1) - (x2, y2)$ in E if and only if $|x1 - x2| = 1$ and $|y1 - y2| = 0$ or $|x1 - x2| = 0$ and $|y1 - y2| = 1$. This reflects the square lattice's grid structure.\newline
		
		The periodic boundary conditions mean that the lattice wraps around in both the horizontal and vertical directions. This can be expressed as follows:\newline
		
		For any lattice site $(x, y)$, there are periodic boundary conditions such that if $x$ is at the edge of the lattice in the horizontal direction, $(x + 1, y)$ is identified with $(x - Lx + 1, y)$, where $Lx$ is the size of the lattice in the $x$-$direction$, and similarly for the vertical direction. This identification allows for the lattice to be considered as a torus, and lattice sites at the boundaries are connected to their counterparts on the opposite edge.\newline
		
		
		\textbf{Definition 1.2.} (Spin-$\frac{1}{2}$ particle) A fermion with spin $S=\frac{(2k-1)}{2}$ for $k \in \mathbb{N}$ equal to 1.\newline
		
		\textbf{Definition 1.4.} (Vertex and Plaquette operators) Given a vertex v and a plaquette p of a torus we can define the following vertex and plaquette operators as tensor products over Pauli operators acting on individual spins indicated by the indices $j \in star(v)$ and $j \in bdy(p)$  \newline 
		
		\begin{center}
			$ Av = \prod_{j \in star(v)} Z_j $ \newline
			
			$ Bp = \prod_{j \in bdy(v)} X_j $.\newline
		\end{center}
		
		
		
		
		
		
    \end{minipage}
        
    
    \begin{minipage}{1\textwidth}
    	
    	\textbf{Definition 1.3.} (Toric Code Hamiltonian) Given v, p, application sites of,  respectively, vertex and plaquette operators, and given spin-$\frac{1}{2}$ particles located on each edge of a torus, we can define the following Hamiltonian for the system\newline
    	
    	\begin{center}
    		
    		$H = -\sum_{v} 
    		Av - \sum_{p} Bp $.\newline
    		
    	\end{center}
    	
    	Recall the following definitions from linear algebra \newline
    	
    	\textbf{Definition 1.5.} (Linear map) Let $V$, $V'$ be the vector spaces over the field K. A linear mapping $F$; $V \rightarrow V'$ is a mapping that satisfies the following two properties \newline
    	
    	1. For any elements $u,v \in V$ we have $F(u + v)=F(u)+F(v)$\newline
    	
    	2. For all c in $K$ and $v \in V$ we have $F(cv)=cF(v)$\newline
    	
    	
    	\textbf{Definition 1.6.} (Hermitian operator) Let $V$ be  a finite dimensional vector space over $\mathbb{C}$, with a fixed positive definite hermitian product defined as $<v,w>$ for $v,w \in V$. Let $A$: $V \rightarrow V$ be a linear map. An operator is called Hermitian (or self-adjoint) if $A^H=A$. This means that for all $u,v \in V$ we have\newline
    	
    	\begin{center}
    		$<Av,w> = <v,Aw>$.
    	\end{center}
    	
    	In particular, it is important for the following appplications to specify that a square matrix $A$ of complex numbers is called hermitian if $(A^*)^T = A$, i.e. if the conjugate-transpose of $A$ is equal to $A$ itself. Note also that for real matrices it is sufficient to compute only the transpose of the matrix to verify hermiticity. \newline
    	
    	\textbf{Definition 1.7.} (Unitary operator) Let $V$ be  a finite dimensional vector space over $\mathbb{C}$, with a positive definite hermitian product. Let $A$: $V \rightarrow V$ be a linear map. We define A to be complex unitary if \newline
    	
    	\begin{center}
    		$<Av,Aw> = <v,w>$.
    	\end{center}
    	
    	for all $v,w \in V$. We can define a complex matrix $A$ to be unitary if $ (A^*)^T=A^{-1}$. We note that it is possible to define an operator to be real unitary, with the only difference that $(A)^T = A^{-1}$. Then, we can also define that a real matrix is unitary if $(A)^T = A^{-1}$ or, equivalently, if $(A)^T A=I$.\newline
    	
    \end{minipage}  	
	
	\begin{minipage}{1\textwidth}
		
		
	    
	    \textbf{Proposition 1.1.} (Properties of $Av$ and $Bp$ operators) $Av$ and $Bp$ operators are Hermitian and involutory, therefore they have eigenvalues $\pm 1$.
	    \newline
	
     	\textit{Proof}\newline
     	
     	We know that the operator $ Av = \prod_{j \in star(v)} Z_j $ and that $ Bp = \prod_{j \in bdy(v)} X_j $ .\newline
     	
     	Firstly, recall the form of the $\sigma_x$ and $\sigma_z$ matrices representing the Pauli gates $X$ and $Z$:
	
	
	    
	    \[
	    \text{Z = $\sigma_z$} =
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix}
	    \]
	    
	 
	    \[
	    \text{X = $\sigma_x$} =
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix}
	    \]
	    
	    
	    Prove that they are Hermitian:\newline
	    
	    \[
	    \text{$( \sigma_z )^{H}$} = 
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix} ^H =
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix}^T =
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix}
	    = \text{$ \sigma_z $}
	    \]
	    
	    
	    \[
	    \text{$( \sigma_x )^{H}$} = 
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix} ^H =
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix}^T =
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix}
	    = \text{$ \sigma_x $}
	    \]\newline
	    
	    Note that Pauli matrices are real matrices, i.e. $A^H=A^T$.
	    Then, knowing that $[\sigma_i,\sigma_j]=0 for i=j$ we can write		\newline
	    
	    \begin{center}
	    $(Av)^{H} = (\sigma_{x_1} \sigma_{x_2} \sigma_{x_3} \sigma_{x_4})^{H} = \sigma_{x_1}^H \sigma_{x_2}^H \sigma_{x_3}^H \sigma_{x_4}^H = \sigma_{x_1} \sigma_{x_2} \sigma_{x_3} \sigma_{x_4} = Av$ \newline
	    
	    $(Bp)^{H} = (\sigma_{z_1} \sigma_{z_2} \sigma_{z_3} \sigma_{z_4})^{H} = \sigma_{z_1}^H \sigma_{z_2}^H \sigma_{z_3}^H \sigma_{z_4}^H = \sigma_{z_1} \sigma_{z_2} \sigma_{z_3} \sigma_{z_4} = Bp$\newline
	    \end{center}
	    
	     Prove the involutory propertyy:\newline
	    
	    \[
	    \text{$( \sigma_z )^{2}$} = 
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix} *
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & -1
	    \end{bmatrix} =
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & 1
	    \end{bmatrix}
	    = \text{$I$}
	    \]
	    
	    
	    \[
	    \text{$( \sigma_x )^{2}$} = 
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix} *
	    \begin{bmatrix}
	    	0 & 1 \\
	    	1 & 0
	    \end{bmatrix} =
	    \begin{bmatrix}
	    	1 & 0 \\
	    	0 & 1
	    \end{bmatrix}
	    = \text{$I$}
	    \]\newline
	    
	    
	     Then, again we can write\newline
	    
	    \begin{center}
	    $(Av)^{2} = (\sigma_{x_1} \sigma_{x_2} \sigma_{x_3} \sigma_{x_4})^{2} = \sigma_{x_1}^2 \sigma_{x_2}^2 \sigma_{x_3}^2 \sigma_{x_4}^2 = I$ \newline
	    
	    $(Bp)^{2} = (\sigma_{z_1} \sigma_{z_2} \sigma_{z_3} \sigma_{z_4})^{2} = \sigma_{z_1}^2 \sigma_{z_2}^2 \sigma_{z_3}^2 \sigma_{z_4}^2 = I$\newline
     	\end{center}
	    
    \end{minipage}    
    
	\begin{minipage}{1\textwidth}
	    
	    
	    As regards the second part of the proposition, we prove that Hermitian operators have real eigenvalues: \newline
	    
	    Write the expression for the eigenvalues $Av |\xi> = \lambda |\xi>$ and take as hypothesis that $|\xi> \neq 0$. Then, by means of the scalar product\newline
	    
     	\begin{center}
	    $<\xi|Av|\xi> = \lambda <\xi |\xi>$\newline
	    
	    $\lambda = \frac {<\xi|Av|\xi>}{<\xi |\xi>}$ = $\frac {<\xi|Av|\xi>}{||\xi||^2}$ = $\frac {<\xi|Av^H|\xi>}{||\xi||^2}$ = $\frac {<\xi|Av|\xi>^*}{||\xi||^2}$ = $\lambda^*$\newline
	    \end{center}
	
	    Note that we applied antidistributivity: \newline
	    
	    \begin{center}
	    $(<\xi|Av^H|\xi>)^H$ = $|\xi>^H (Av^H)^H <\xi|^H$ = $<\xi|Av|\xi>^*$. \newline
	    \end{center}
	
	    Using hermiticity with the fact that $(Av)^2=I$ we can derive the unitarity of $Av$ and state that $Av Av^H = Av^H Av = (Av)^2 = I$, i.e. for a unitary operator $U$ we have $U^H=U^{-1}$. \newline
	    
	    One property of unitary operators states that their eigenvalues have modulus equal to one:\newline 
	    
	    
	    By taking as hypothesis $|\xi> \neq 0$, we write the expression $U |\xi> = \lambda |\xi>$ and its self-adjoint $<\xi| U^H = \lambda^* <\xi|$. Then, knowing that $U^H=U^{-1}$, by means of the scalar product we obtain \newline
	    
	    \begin{center}
	    $<\xi|U U^H|\xi> = \lambda \lambda^* <\xi |\xi>$\newline
	    
	    $<\xi|I|\xi> = \lambda \lambda^* <\xi |\xi>$\newline
	    
	    $<\xi|\xi> = \lambda \lambda^* <\xi |\xi>$\newline
	    \end{center}
	    
	    Finally, since we already know that Hermitian operators have real eigenvalues we can write $|\lambda|^2 = 1$.\newline
	    
	    Putting together the fact that $Av$ has real eigenvalues with unitarity we obtain that the only two remaining possibilities for the eigenvalues of $Av$ are $\pm 1$. \newline
	    
	    The same reasoning can be carried out for the $Bp$ operator.
	    
	    \hfill $\square$
	    
	    
	\end{minipage}
	
	\begin{minipage}{1\textwidth}
		
		
		\textbf{Proposition 1.2.} (Spectrum of $Av$ and $Bp$ operators) The spectrum of $Av$ and $Bp$ operators is $\{-1,+1\}$. \newline
		
	    
    	\textbf{Proposition 1.3.} (Commutation of $Av$ and $Bp$ operators) The operator Av commutes with the operator Bp for an even number of edges.
    	\newline
    	
    	\textbf{Proposition 1.4.} (Anticommutation of $Av$ and $Bp$ operators) The operator $Av$ anticommutes with the operator $Bp$ for an odd number of edges.
    	\newline
    	
    	\textit{Proof}\newline 
    	%Av commutes with itself.\newline
    	%Bp commutes with itself.\newline
    	%Av commutes with Bp for an even number of edges.\newline
    	
    	Fix the origin of the coordinate system in the bottom left corner of the lattice as indicated in the picture below:\newline
    	
    	%picture
    	
    	then, we can define the two vectors representing the site of application of the vertex and plaquette operator, respectively over the lattice L and dual lattice L'
    	
    	\begin{center}
    		$\vec{v}$= $n\hat{e_1} + m\hat{e_2}$, where $n,m \in \mathbb{Z}$ \newline
    		
    		$\vec{p}$= $(n + \frac{1}{2}) \hat{e_1} + (m + \frac{1}{2}) \hat{e_2}$, where $n,m \in \mathbb{Z}$\newline
    	\end{center}
    	
    	Rewrite the operators as follows:\newline
    	
    	\begin{center}
    		
    		$A_{\vec{v}} = \sigma^z_{\vec{v}+\frac{1}{2}\hat{e_1}} \sigma^z_{\vec{v}+\frac{1}{2}\hat{e_2}} \sigma^z_{\vec{v}-\frac{1}{2}\hat{e_1}} \sigma^z_{\vec{v}-\frac{1}{2}\hat{e_2}}$ \newline
    		
    		$B_{\vec{p}} = \sigma^x_{\vec{p}+\frac{1}{2}\hat{e_1}} \sigma^x_{\vec{p}+\frac{1}{2}\hat{e_2}} \sigma^x_{\vec{p}-\frac{1}{2}\hat{e_1}} \sigma^x_{\vec{p}-\frac{1}{2}\hat{e_2}}$\newline
    	
    	\end{center}
    	
    	In order to simplify the calculations we rewrite $Bp$ on the lattice L by rewriting the indeces in terms of vector $\vec{v}$
    	
    	\begin{center}
    		$(n\hat{e_1} + m\hat{e_2}) + \frac{1}{2}\hat{e_2}= n\hat{e_1} + (m+\frac{1}{2}\hat{e_2})$\newline
    		
    		$(n\hat{e_1} + m\hat{e_2}) + \frac{1}{2}\hat{e_1}= (n+ \frac{1}{2})\hat{e_1} + m\hat{e_2}$\newline
    		
    		$(n\hat{e_1} + m\hat{e_2}) + \frac{1}{2}\hat{e_1}+\hat{e_2}= (n+ \frac{1}{2})\hat{e_1} + (m + 1)\hat{e_2}$\newline
    		
    		$(n\hat{e_1} + m\hat{e_2}) + \frac{1}{2}\hat{e_2}+\hat{e_1}= (n+ 1)\hat{e_1} + (m + \frac{1}{2})\hat{e_2}$\newline
    	\end{center}
    	
    	Then the $B_{\vec{p}}$ operator becomes:
    	
    	\begin{center}
    		
    		$B_{\vec{v}} = \sigma^x_{n\hat{e_1} + (m+\frac{1}{2}\hat{e_2})} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + (m + 1)\hat{e_2}} \sigma^x_{(n+ 1)\hat{e_1} + (m + \frac{1}{2})\hat{e_2}}$\newline
    		
    	\end{center}
    	
    	and the Hamiltonian can be written by grouping the indices:\newline
    	
    	\begin{center}
    	
    	$H = - \sum_{m,n \in \mathbb{Z}} \{ 
    	\sigma^z_{(n+\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m+\frac{1}{2})\hat{e_2}} \sigma^z_{(n-\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m-\frac{1}{2})\hat{e_2}} +
    	\sigma^x_{n\hat{e_1} + (m+\frac{1}{2}\hat{e_2})} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + (m + 1)\hat{e_2}} \sigma^x_{(n+ 1)\hat{e_1} + (m + \frac{1}{2})\hat{e_2}} \} $
    	
       \end{center}
    	
	
    \end{minipage}
	
	\begin{minipage}{1\textwidth}
		
		Now calcuate the commutator $[A_{\vec{v}},B_{\vec{v}}] = A_{\vec{v}}B_{\vec{v}} - B_{\vec{v}}A_{\vec{v}}$ by focusiing on the first term:\newline
		
		\begin{center}
			
			$ A_{\vec{v}}B_{\vec{v}} =
			\sigma^z_{(n+\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m+\frac{1}{2})\hat{e_2}} \sigma^z_{(n-\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m-\frac{1}{2})\hat{e_2}} $ *
		
			$\sigma^x_{n\hat{e_1} + (m+\frac{1}{2}\hat{e_2})} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + (m + 1)\hat{e_2}} \sigma^x_{(n+ 1)\hat{e_1} + (m + \frac{1}{2})\hat{e_2}}$\newline
			
		\end{center}
		
		
		Matrices do not commute but for Pauli matrices we have the following commutation relationship :\newline
		
		
		\begin{center}
			$\sigma^x_{\vec{v}}\sigma^z_{\vec{v}'} = \sigma^z_{\vec{v}'} \sigma^x_{\vec{v}} + 2* \sigma^x_{\vec{v}}\sigma^z_{\vec{v}'} \delta_{\vec{v} \vec{v}'}$\newline
	    \end{center}
	    
		
		where $\hspace{1cm} \delta_{\vec{v} \vec{v}'} =$
		$\begin{cases}
			1, \hspace{1cm} if \hspace{1cm}  \vec{v} = \vec{v}'\\
			0, \hspace{1cm} if \hspace{1cm} \vec{v} \neq \vec{v}'
		\end{cases}$\newline
		
		which states that 	$\sigma^x_{\vec{v}}\sigma^z_{\vec{v}'}$ commutate for $\vec{v} \neq \vec{v}'$   but anticommutate for $\vec{v} = \vec{v}'$. This is known from the anticommutation relationship of Pauli matrices $\sigma^x \sigma^z = - \sigma^x \sigma^z$. Thus, for an even numer of overlapping edges, in our case 2 or 4, the commutator becomes:\newline
	
	    \begin{center}
	    	
	    	$[A_{\vec{v}},B_{\vec{v}}] = 2 *
	    	\sigma^x_{n\hat{e_1} + (m+\frac{1}{2}\hat{e_2})} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^x_{(n+ \frac{1}{2})\hat{e_1} + (m + 1)\hat{e_2}} \sigma^x_{(n+ 1)\hat{e_1} + (m + \frac{1}{2})\hat{e_2}}*
	    	\sigma^z_{(n+\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m+\frac{1}{2})\hat{e_2}} \sigma^z_{(n-\frac{1}{2})\hat{e_1} + m\hat{e_2}} \sigma^z_{n\hat{e_1}+(m-\frac{1}{2})\hat{e_2}} $ 	\newline 
	    
	    	
	    \end{center}
	    Instead, for an odd number of edges it becomes null $[A_{\vec{v}},B_{\vec{v}}]=0$.\newline
	    
	    This calculations conclude that $A_{\vec{v}},B_{\vec{p}}$ commute for an even numer of edges but anticommute for an odd number of edges.\newline
		
		\hfill $\square$\newline
		
		
	\end{minipage}
	
	
	
	
	\begin{minipage}{1\textwidth}
		
		\textbf{Proposition 1.5.} (Ground state(s) of the Hamiltonian) The ground states of the Hamiltonian are the simultaneous $+1$ eigenstates of all the $Av$ and $Bp$ operators. \newline
		
		\textit{Proof}\newline 
		
		Recall the form of the Hamiltonianin, having N lattice sites, the following way \newline
		
		\begin{center}
			
			$H = -\sum_{i=1}^{N}
			Av_i - \sum_{j=1}^{N} Bp_j $.\newline
			
		\end{center}
		
		we already know that Hermitian matrices are simultaneously diagonalizable, therefore we can determine the possible eigenvalues for each of the $Av$ and $Bp$ operators. From Proposition 1.2. we know that such eigenvalues can assume two values $\{-1,+1\}$ due to the specific properties of our operators.\newline
		
		To determine the Ground state(s) of the Hamiltonian we have to determine the minimum energy of the system. Therefore we compute all the possible combinations of the eigenvalues associated to the vertex and palquette operators to obtain the following spectrum of values\newline
		
		\begin{center}
		$\sigma( H) =$
		$\begin{cases}
			2N, \hspace{1cm} if \ all\ Av,Bp \ have \ \lambda_{H}= -1\\
			2N-1,\\
			.\\
			intermediate \ energies,\\
			.\\
			-2N+1\\
			-2N, \hspace{1cm} if \ all \ Av,Bp \ have \ \lambda_{H}= +1
		\end{cases}$
		\newline
     	\end{center}
		
		Taking the minimum of this spectrum means $\sigma(H)=-2N$, thus considering only eigenstates for the Hamiltonian associated to $+1$ eigenvalues.\newline
		
		Those will form a basis for the Ground State manifold of the system.\newline
		
		\hfill $\square$\newline
		
		
		Now we focus on determining the form of such ground state(s).\newline
		
	\end{minipage}
	
	
	\begin{minipage}{1\textwidth}
		
		In order to determine which are the admissible configurations that we can use to form the Ground State(s), we treat the $Av$ and $Bp$ operators as constraint equations over the Torus, i.e. the lattice and dual lattice.\newline
		
		All of our configurations will need to respect the following equations:\newline
		
		\begin{center}
		 (1)	$Av|\psi>$ = $+1|\psi>$
		\end{center}
		
		\begin{center}
		 (2)	$Bp|\psi>$ = $+1|\psi>$
		\end{center}
		
		This means that configuration $|\psi>$ needs to be be an eigenvector for both $Av$ and $Bp$ operators. \newline
		
		In order to satisfy equation (1), we look for loop configurations such that if we apply $Av$ to that state, the result would still yield a positive eigenvalue.\newline
		
		Graphically, we identify the loop configurations through strings of 'occupied' edges (here shaded in black), each identified by $|1>$. Then,if we apply $Av$ at one of the open ends of such strings, we have two possibilities: leaving the string of occupied edges open or closing the string over the $Av$ operator (here highlighted in blue).\newline
		
		%drawings
		
		In the first case, what we do is computing $\sigma^{z} |1>$, which implies obtaining $-1|1>$ as a result, thus violating contraint (1). The second case is a natural consequence following the first result. Thus, in order to respect contraint (1) we are interested only in closed loops. notice that such loops will always have an even length and will always involve a maximum of two extremities of the $Av$ operator.\newline
		
		In a more formal notation what we are stating is that:\newline
		
		\begin{center}
			$\prod_{i=1}^{4} \sigma_{i}^{z} |\psi> = +1 |\psi>$. \newline
		\end{center}
		
		Considering only closed loops, we identify different configurations having such characteristic. The below illustrations show some examples of them:\newline
		
		%drawings
		
		Each of these loops is an eigenstate for $Av$, since no matter how I locally apply $Av$, I will always preserve the sign of the state. \newline
	
		
		
		
	\end{minipage}
	
	
	
	\begin{minipage}{1\textwidth}
		
		Then we focus on constraint (2). We want to apply the plaquette operator $Bp$ only to eigenstates of $Av$, which we have determined above. The illustration below shows how to do it with one of the loops above:\newline
		
		%drawings
	
		
		Notice that, after the transformation, we do always end up in a valid eigenstate of $Av$ but the new state $|\psi'>$ is not an eigenstate of $Bp$ by itself.\newline
		
		Furthermore, any new configuration that we obtain through the application of the plaquette operator to an eigenstate of $Av$ simply yields one of the possible permutations of the edges of the initial state, provided that the topological characteristics of the loop are preserved.\newline
		
		This means that if we firstly partition the eigenstates of $Av$ in the following fours classes: \newline
		
		- class 0 : contains all closed loops and thus the vacuum state,
		since all of the closed loops can be continuously deformed into a null state; \newline
		
		- class 1 : contains loops that wind all the way around the horizontal dimension of the torus and their permutations;\newline
		
		- class 2 : contains loops that wind all the way around the vertical dimension of the toru and their permutations;\newline
		
		- class 3 : contains loops that wind all the way around both dimension of the Torus and their permutations; notice that the vertical loop must be taken on the dual lattice to have a valid configuration.\newline
		
		Then, applying a plaquette operator to any of the eigenstates belonging to one of the classes above must yield an eigenstate that lies in the same class. This is formally expressed by stating that the class is invariant under the action of the operator $Bp$. \newline
		
		\textbf{Proposition 1.6.} (Invariance under $Bp$) The four classes of eigenstates of $Av$ are invariant under the action of the $Bp$ operator. \newline
		
		We can prove that the above definition holds by means of counterexamples.\newline
		 
		If we take the loop illustrated below, we would be brought to believe that there indeed exist a way to apply the operator $Bp$ such that we exit class zero and land in class 1. \newline
		
		%drawings
		
		In order to show the impossibility of the above action, we define two topological indeces to label the four categories of eigenstaes, which can assume values in $\mathbb{Z}_2=\{\overline{0},\overline{1} \}$. These elements respectively represent the 'sets' of even and odd numbers. In our context such numerosities identify the number of vertical and horizontal loops trapassing the dimensions of the Torus. \newline
		
		The above indecs can also be expressed as follows: 
		
		\begin{center}
			$n_x= (number \ of \ vertical \ intersections)mod2 = 
			\begin{cases} 
				0mod2 \\
			    1mod2  
			\end{cases}$ 
			$n_y= (number \ of \ horizontal \ intersections)mod2 =\begin{cases} 
				0mod2 \\
				1mod2  
			\end{cases}$ 
		\end{center}
		
		
	
	\end{minipage}
	
	
	
	
	
	\begin{minipage}{1 \textwidth}
		
		in order to create a correspondence with the actual definition of the Torus over $(\mathbb{Z}$ x $\mathbb{Z})$. 
		We only change the representatives of the elements $\{\overline{0},\overline{1} \}$ in the intermediate step. \newline
		
		So, in total our classes are labelled as : $(\overline{0},\overline{0} )$, $(\overline{0},\overline{1} )$, $(\overline{1},\overline{0})$, $(\overline{1},\overline{1})$.\newline
		
		Notice that the intersections are meant to be computed by fixing two circles: one horizontal circle passing through the spins (the princiapla lattice) and one vertical circle passing in the middle of the lattice plaquettes, i.e. taken on the dual lattice.\newline
		 
		For example if we take the below configuration: \newline
		
		%drawing
		
		We have 1 vertical intersection and 1 horizontal intersection, therefore we are in the class three indexed by $n_x=1mod2$ and $n_y=1mod2$, which is labelled as $(\overline{1},\overline{1})$. This would have been true for any odd number of vertical and horizintal intersections, since they all fall in the set of numbers given by $1mod2$.\newline
		
		Going back to the initial example, this would mean that we land not in class 1, but in class 0, as we have an even number of horizontal intersections and an even number of vertical intersections, i.e.   $(\overline{0}, \overline{0})$.\newline
		
		Thus, all the classes are $Bp-invariant$ due to the topological characteristics of the Torus.\newline
		
		More formally we can now write that, given a set of eigenstates of $Av$ named $|\psi_1>,...,|\psi_i>,...,|\psi_n>$ all belonging to the same class and forming state $|\psi>$. Given that we know that $Bp|\psi_i>=+|\psi_j>$, then if we apply $Bp$ to the normalized state $|\psi>$ then we get:
		
		\begin{center}
			$|\Xi>= Bp \frac{1}{\sqrt{n}} \sum_{j=1}^{n} |\psi_i> = \frac{1}{\sqrt{n}} \sum_{j=1}^{n} Bp |\psi_i> = \frac{1}{\sqrt{n}} \sum_{j=1}^{n} |\psi_J> =|\Xi>$
		\end{center}
		
		because what $Bp$ does is only permuting the $|\psi_i>$, thus we get back our initial state. \newline
		
		Which leads to the following Proposition:\newline
		
		\textbf{Proposition 1.7.}(Eigenstates of $Bp$) Eigenstates of $Av$ are not eigenstates of $Bp$ by themselves but completely symmetric superspositions of any of them.\newline
		
		From Proposition 1.6 and Proposition 1.7, naturally follows the degeneracy (and dimension) of the Ground State:\newline

		\textbf{Proposition 1.8.} (Degeneracy of the Ground State manifold) The degeneracy of the Ground State manifold is $4$. \newline
		
		As the four classes of eigenstates are $Bp-invariant$, then we can construct a valid Ground State through the configurations belonging to one the four classes, as these configurations respect both contraint (1) and (2).\newline
		
	\end{minipage}
















 
	
	\begin{minipage}{1 \textwidth}
		\textbf{Exchange statistic in 3D }\newline
		
		Let us first consider the exchange statistics of two particles in three dimensions.  We can describe a system of two particles moving from point $(r_1(t_1),r_2(t_1))$ to point $(r'_1(t_2),r'_2(t_2))$ through the following path integral formulation 
		
		\begin{center}
			$A$ = $\sum_{paths} e^{iS}$
		\end{center}
		
		where S represents the integral of the Lagrangian $L$ over time, which provides a measure of the total "action" along a particular trajectory or path taken by the system. 
		
		\begin{center}
			$S = \int_{t_1}^{t_2}dt \textit{L} [ r_1(t),r_2(t) ] $
		\end{center}
		
		In quantum mechanics, the path integral formulation involves considering all possible paths between the initial and final states and assigning a phase factor $e^{iS}$ to each.\newline
		
		Then, the probability amplitude for a system to evolve from one point to another is given by the sum (or integral) over all paths of these phase factors. Though, in order to obtain the actual probability of the system evolving between the two points we should compute:
		
		\begin{center}
			$P= |A|^2 $
		\end{center}
		
		Overall, we have:\newline
		
		-Probability Amplitude $A$: It is a complex number that encodes both the magnitude and phase information associated with a quantum process. The probability amplitude is used in quantum mechanics, particularly in the context of the path integral formulation, to describe the evolution of a system and calculate probabilities.\newline
		
		-Probability ($P$): It is a real number representing the likelihood of a particular outcome or event. In quantum mechanics, the probability is obtained by taking the squared magnitude of the probability amplitude. Mathematically, this ensures that the probability is a non-negative real value. So, P represents how likely it is that our system of two particles will evolve from a point at t1 to a point at t2.\newline
		
		Since we are talking about a system of indistinguishable particles, we will only have two classes of paths in three dimensions, the direct paths and paths with exchanges as pictured below.\newline
		
		%picture
		
	    This is because, as we can see above, the final configuration at time $t_2$ will always be the same wheter we end up having $(r_1(t_2),r_2(t_2))$ or $(r_2(t_2),r_1(t_2))$, indeed due to indistinguishability.\newline
		
		Though, notice that even though two paths may lead to the same final configuration they can exhibit different behaviors when it comes to considering the relative coordinates. By defining the center of mass $R= \frac{r_1+r_2}{2}$ , it becomes evident that the center of mass motion remains consistent for both paths, while the relative coordinate $r=r_2-r_1$ undergoes distinct changes.\newline
		
		To visualize this in configuration space, consider maintaining the magnitude of the relative coordinate $|r|$ fixed and non-zero, implying that the particles do not intersect. \newline 
		
	
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		In such a scenario, paths are constrained to move along the surface of the sphere due to the fixed magnitude of the relative coordinate. Closed paths on the sphere emerge when particles return to their original positions (no exchange) or reach the antipodal point (exchange). If particles are exchanged a second time, $r$ completes a full revolution around the sphere. \newline
		% in three-dimensional space, a vector with a fixed magnitude can be represented as a point on the surface of a sphere.
		
		Eliminating coincident points ensures that the wave-function remains non-singular and well-defined across all points in configuration space, particularly on the surface of the sphere. Consequently, the phase acquired by the wave-function remains well-defined and invariant under continuous deformations of the path.\newline
		
		Examining the possible phases of the wave-function along three distinct paths: A (no exchange), B (single exchange), and C (two exchanges), as shown in the figure below, we discover distinct characteristics: \newline
		
		-Path A: being a closed loop without exchange, it can be continuously shrunk to a point, implying that the wave-function cannot acquire any phase other than unity.\newline 
		
		-Path B: involving a single exchange, it connects two fixed points on the sphere, making it impossible to shrink to a single point. Consequently, this exchange introduces a non-trivial phase in the wave-function. \newline
		
		-Path C: forming a closed loop with two exchanges, it can be continuously shrunk to a point by envisioning the path as a physical string looped around a sphere. As a result, path C does not acquire any additional phase.\newline
		
		Overall, we only have two classes of paths, those that do not involve any exchange as in class A and C, and those that are characterised by an exchange as in class B.\newline
		Then, if we let $\eta$ represent the phase acquired by the wave function under a single exchange. Since two exchanges are equivalent to no exchange $\eta^2=1$, then it follows that $\eta = \pm 1 $. Therefore, the only possible statistics in three dimensions are Fermi statistics or Bose statistics.\newline
		
		In terms of the path integral formulation that we have provided above, what we will get is thus:
		
		\begin{center}
			$A[r_1(t_1),r_2(t_1) \rightarrow r'_1(t_2),r'_2(t_2)]$ = $\sum_{direct \ paths} e^{iS}$ + $\sum_{exchange \ paths} e^{iS}$
		\end{center}
		
		In terms of the path integral, we can also introduce a phase
		between the two classes of paths and write: 
		
		\begin{center}
			$A[r_1(t_1),r_2(t_1) \rightarrow r'_1(t_2),r'_2(t_2)]$ = $\sum_{direct \ paths} e^{iS}$ + $e^{i \phi} \sum_{exchange \ paths} e^{iS}$
		\end{center}
		
		Since exchanging the particles twice leads again to a direct path, we will have $e^{2i \phi} = 1$. Solving this equation, implies that $\phi$ can only be 0 or $\pi$ giving rise to bosons and fermions. \newline	
		
		-$\phi = 0$: This case leads to the constructive interference of the exchange paths. The resulting particles are known as bosons, and their behavior is characterized by this specific phase condition.\newline
		
		-$\phi = \pi$: This case results in destructive interference between the exchange paths. The particles corresponding to this scenario are referred to as fermions, and their behavior is governed by this particular phase condition.\newline
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		\textbf{Exchange statistic in 2D }\newline
		
		In two dimensions the topology of the space configuartion changes. That is, by proceeding in the same way as we did in three dimensions, thus by fixing the magnitude of the relative coordinate we end up moving on a circle.\newline
		
		Though, in this case several possible paths are possible, particularly closed paths.
		If we look at the three classes of paths A,B,C we see that :\newline
		
		-Path A: can be shrunk into a point as it simply moves on the circle, as exemplified in the picture below. \newline
		
		-Path B: cannot be contracted into a point as the endpoints remain fixed. \newline
		
		-Path C: also this kind of paths cannot be shrunk into a point since they wind all  the way around the circle. In three dimensions, the path that forms under two exchanges can be shrunk to a point. This is not possible if the motion is restricted to a plane.\newline
		
		%picture
		
		
		Let $\eta$ represent the phase associated with a single exchange, $\eta^2$
		denote the phase under two exchanges, and $\eta^3$ signify the phase under three exchanges, and so forth. The crucial observation is that, given that the wave-function's modulus remains constant during exchanges, we can express $\eta$ as a phase factor $e^{i\theta}$. \newline
		
		In terms of path integrals what we get as probability amplitude is: \newline
		
		\begin{center}
			$A$ = $\sum_{direct \ paths} e^{iS}$ + $e^{i \phi} \sum_{ one \ exchange} e^{iS}$  + $e^{2i \phi} \sum_{ two \ exchanges} e^{iS}$  + ...
		\end{center}
		
		where for $ \phi = 0, \pi$ we would obtain the usual bosons and fermions statistics, but since in general, $e^{in\phi} \neq 1$ for any $n$ (n exchanges never yield the identity), $\phi$ can be anything so any statistic is possible in two dimensions.\newline
		
		This elucidates the flexibility in obtaining diverse statistics in two dimensions, which are called 'anyonic'.\newline
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		\textbf{Anyons obey Braid Group Statistics}\newline
		
		In a formal mathematical context, it can be articulated that the examination presented in the initial section categorizes particles within the realm of the permutation group $\mathbb{P}_N$. Conversely, the subsequent analysis in paragraph two places particles under the classification of the braid group $\mathbb{B}_N$. \newline
		
		The permutation group $\mathbb{P}_N$ is defined as the collective set of all conceivable permutations involving $N$ objects. Within this group, the operation of group multiplication (exchange) corresponds to the sequential application of permutations, and the group inverse involves the reversal of the permutation.
		Here, executing a permutation twice on two objects results in the restoration of the system to its initial configuration. Consequently, particles exhibiting transformations in accordance with representations of the permutation group can exclusively fall into the categories of either bosons or fermions. \newline
		
		On the other hand, when it comes to objects that are part of the braid group $\mathbb{B}_N$, we can visualise the process of moving (or exchanging) particles as paths in spacetime with time being the vertical axis and space being the horizontal axis. The particles can circle around each other and form closed paths by coming back to their original positions. \newline
		
		In a formal sense, we define the braid group $\mathbb{B}_N$ as the collection of diverse trajectories that arise during the adiabatic transport of $N$ particles. These trajectories depict the evolution of a configuration of $N$ particles from a specific time, denoted as 0, to another time, denoted as $t$. It is essential that the world lines of the particles neither intersect nor create entangled structures or loops. Maintaining a consistent count of $N$ particles at each moment is crucial. Consequently, every set of trajectories followed by the $N$ particles can be characterized as a braid. \newline
		
		%pictures
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		\textbf{Toric Code : low-energy excitations}\newline
		
		We shall now examine the case of a square lattice with boundary conditions, i.e. the toric code. \newline
		
		Taking into account the results yielded by the examination of the physical system in the previous chapters (especially the four fold degeneracy) we will now proceed to firstly describe what are the excitations of the ground state, what kind of excitations we can obtain on the ground state, identify their respective statistics and ultimately we will examine the exchange statistics of the excitations.\newline
		
		In general, the low-energy excitations of a quantum system, often referred to as quasiparticles, represent collective deviations from the system's ground state. These excitations are associated with elementary quantum modes that can be created or annihilated with relatively low energy. The properties of these quasiparticles, including their statistics, are crucial for gaining insights into the system's ground state degeneracy, topological properties, and response to external perturbations. \newline
		
		In the particular case of the Toric Code, such low-energy excitations can be created in two ways:  \newline
		
		1. by applying an open-ended string $S^x$ made up of $\sigma_x$ operators on the Ground State;\newline
		
		2. by applying an open-ended string $S^z$ made up of $\sigma_z$ operators on the Ground State;\newline
		
		At the extremities of the above mentioned strings particles are created. We will call the pairs of particles obtained through the application of operator $S^x$ "electric charges", while the ones obatained by applying an $S^z$ operator to the Ground State "magnetic vortices".\newline
		
		%picture
		
		We can prove that if a string $S^x$ of $\sigma_x$ operators is open-ended, then the $\sigma_x$ operators at its two extremes of the string will anticommute with one $A_v$ each.\newline
		
		\textit{Proof}\newline 
		
		We want to show that $A_v S^x + S^x A_v=0$.\newline
		
		Knowing that 
		
		\begin{center}
			$A_v = \prod_{i=1}^{4} \sigma_i^z$ \\ 
			$S^x = \prod_{j=1}^{N} \sigma_j^x$
		\end{center}
		
		by substitution we obtain the following expression: \newline
		
		\begin{center}
			$\prod_{i=1}^{4} \prod_{j=1}^{N} \sigma_j^x + \prod_{j=1}^{N} \sigma_j^x \prod_{i=1}^{4} \sigma_i^z = 0$ $(1)$
		\end{center}
		
		remeber that only one $\sigma_i^z$ will overlap with the extremity of the string $S^x$; thus, only one $\sigma_i^z$ will anticommute with the extremity of the string $S^x$. Knowing that for Pauli matrices acting on the same edge we have anticommutation
		
		
		
	\end{minipage}
	
	
	\begin{minipage}{1 \textwidth}
		
		\begin{center}
			$\sigma_{v'}^x \sigma_{v'}^z = - \sigma_{v'}^z \sigma_v^x$ for $v=v'$ \\
			$\sigma_v^x \sigma_{v'}^z =  \sigma_{v'}^z \sigma_v^x$ for $v \neq v'$ 
		\end{center}
		
		if for simplicity we fix $N=2$ we can easily see that :\newline
		
		$(\sigma_1^z \sigma_2^z \sigma_3^z \sigma_4^z)(\sigma_1^x \sigma_2^x)  = - (\sigma_1^x \sigma_2^x)(\sigma_1^z \sigma_2^z \sigma_3^z \sigma_4^z) $ \newline
		
		having supposed that, for example, the extremity $\sigma_1^x$ overlaps with the $\sigma_3^z$ edge part of the $A_v$ operator. This can then be easily generalized for $N$ Pauli operators. Finally, if we substitute such expressione in $(1)$ we obtain that the overall equation is in fact equal to zero.\newline
		
		Of course, this procedure should be repeated also for the other extremity of the string $S^x$ to show that the operator commutes with two $A_v$ operators, one for each endpoint.\newline
		
		\hfill $\square$ \newline
		
		Similarly, if we take a string $S^z$ made up of $\sigma_z$ operators still open-ended, we can prove that the $\sigma_z$ operators at its endpoints will anticommute with one $B_p$ each.\newline
		
		\textit{Proof}\newline 
		
		\hfill $\square$ \newline
		
		The effect of putting such strings on the Ground State correspond to raising the associated energy of the GS by 2. In fact, we can prove that it is impossible to create unitary excitations on the Ground State.\newline
		
		\textit{Proof}
  Recall that for a string operator, here we will choose $S^x$, both equations hold:
		
		\begin{center}
			$\begin{cases} 
				A_{v1} S^x + S^x A_{v1} =0 \\
				A_{v2} S^x + S^x A_{v2} =0
			\end{cases}$ 
	    \end{center}
		
		If we apply the string operator to the Ground State, taking into account anticommutativity stated above, we obtain that:
		
		\begin{center}
			$\begin{cases}
				A_{v1} S^x |GS> = - S^x A_{v1} |GS> = - S^x |GS> \\
				
				A_{v2} S^x |GS> = - S^x A_{v2} |GS> = - S^x |GS> 
			\end{cases}$ 
	    \end{center}
		
		since $A_{v1}|GS> = +1|GS>$ and $A_{v2}|GS> = +1|GS>$.
		Summing up term by term :\newline
		
		\begin{center}
			$\begin{cases}
				A_{v1} S^x |GS> + A_{v2} S^x |GS> = - 2 S^x A_{v1} |GS> 
			\end{cases}$ 
		\end{center}
		
		Thus, if the energy of the Ground State as we have defined it in the previous chapters is $-2N$ by acting with a string operator on  it we ibtain $-2N+2$.
		
		\hfill $\square$ \newline
		
		
	\end{minipage}
	
	
	\begin{minipage}{1 \textwidth}
 
		\textbf{Statistics and Braiding properties of the excitations}\newline
		
		Firstly, we study the individual statistics of electric charges and magnetic votices, then we argue their mutual statistics.\newline
		
		Let's analyze the statistics of e-particles in the context of the ground state. The operator $\sigma^z|GS>$ , where $|GS>$ denotes the ground state, creates two e-particles in the vertices adjacent to the edge. The presence of a string operator $\prod_{j=1}^{N} \sigma_j^z$ allows the separation of these two excitations. No only, ass illustrated below, the exchange of these particles becomes possible by applying the string operators. However, due to the commutation of all $\sigma^z$ with each other, regardless of the path taken during the exchange, there is no acquired phase. Consequently, the e-excitations exhibit bosonic statistics. \newline
		
		%picture
		
		A similar argument can be made for the m-excitations, establishing them as bosons as well.
		
		Let's examine the mutual statistics between e and m particles. Initially, we generate pairs of e and m excitations at distinct chosen sites by applying the operator $\sigma^z\sigma^x|GS>$. Afterward, we separate the excitations using string operators and we move an m-particle around an e-particle, as depicted below. Notably, there is a site where $\sigma^z$ must pass beyond a $\sigma^x$ spin, resulting in an anticommutation and thus introducing a minus sign. The closed loop can be eliminated, leading to the following transformation:
		
		\begin{center}
			$\sigma_i^z\sigma_j^x|GS> = - \sigma_i^x\sigma^z|GS>$
		\end{center}
		
		if we denote the exchange operator as $R$ then the wave function relation should be the following:
		
		\begin{center}
			$R^2\psi(e,m) = - \psi(e,m)$
		\end{center}
		
		since we are going all the way around the particle, we assume to have two exchanges. Then we simply find out that:
		
		\begin{center}
			$R^2 = - 1 \rightarrow ( e^{-\frac{i\pi}{2}})^2 = - 1  \rightarrow R = e^{-\frac{i\pi}{2}} = \pm i $
		\end{center}
		
		this results falls out of the usual bosonic/fermionic statistics, thus we can say that the low-energy excitations of the ground state exhibit anyonic statistics.\newline
		
	\end{minipage}
	
	
	\begin{minipage}{1 \textwidth}	
	\end{minipage}
	

















 
	\chapter{Coding}
	% Altre sezioni
	\begin{minipage}{1 \textwidth}
		
		%1
		\textbf{0. Introduction}\newline
		
		%4
		\textbf{1. Classical Error-Correction}\newline
		
		In Classical Computing the most elementary unit of information takes the name of bit. A bit stores binary information, meaning that it can either assume value 1 or 0. When we transmit information, we immagine to make strings of bits travel through a channel as the one depicted below: \newline
		
		%image
		
		
		Though, a bit can change its value during the transmission, either going from 1 to 0 or viceversa, this event is called a bit-flip and it tells us that an error occured in the transmission. A bit-flip can occur independently on each bit with a certain probability that we will denote as $p << 1$. In such a way we expect each bit to be corrupted after $O(\frac{1}{p})$. \newline
		
		The noisy channel described as such, is one of the easiest error models also known as the $\textit{Binary Symmetric Channel}$. An error model, in the context of information theory and communication, is a mathematical or conceptual representation of how errors can occur in the transmission or processing of data. It describes the types of errors that can happen, the probabilities associated with each type of error, and the conditions under which these errors occur. \newline
		
		In order to protect information we introduce redundancy, this heps us detecting errors and correcting them. 
		Introducing redundacy means duplicating bits of infromation before sending them through a channel. Formally we say that we encode information into bit strings, for example: 0 is encoded into 00 and 1 into 11. This is useful to detect errors because in this way, whenever a bit changes we know that an error has occured, since 10 and 01 pairs should never occur. Formally we have introduced a parity: the strings 00 and 11 have even parity while 10 and 01 have odd parity. Whenever we catch a string with odd parity, we have an error.		\newline
		
		Though, in order to be able to also correct errors the redundancy above is not enough since we wouldn't be able to define a criterion for which correcting the corrupted information. Thus, we introduce a third redundant bit; in this way, when we decode information at the destination, we can apply what is called as majority voting. Majority voting decodes into ones those triplets that have most bits set to 1 and decodes as zeros the ones that have most bits set to 0.
		
	
		
	\end{minipage}
	
	
	
	\begin{minipage}{1 \textwidth}
		
		For example, if we receive 010 we will decode it into 0. \newline
		We can still define a parity rule; in this case, we look at two parities: the parity
		of the first two bits, and the parity of the second two bits.  For example 000 and 111 have parity 00 while all the other combinations have at least one bit set to 1.  When we check for parity we perform what is called as parity-check and we call the obtained values error syndromes. If we know the latter, we know what error has occured and how to correct it.\newline
		
		00 no error (coresponds to codeword 000 or 111);\newline
		01 3rd bit flipped (coresponds to codeword 001 or 110);\newline
		10 1st bit flipped (coresponds to codeword 100 or 011);\newline
		11 2nd bit flipped (coresponds to codeword 010 or 101).\newline
		
		The above Redundant Code is actually an example of what is called a Linear code.
		We can in fact reframe all of the above in terms of linear algebra.\newline
		
		To do so we define what is called the Generator matrix  \textbf{G}, which is the one that we use to introduce redundancy. \textbf{G} encodes the original information into codewords. For example, if we have a single bit to be encoded in a three bit codeword we can do the following: \newline 
		
		\begin{center}
			\textbf{G} = $[1 \ 1 \ 1]^{T}$  
		\end{center}
		\begin{center}
			a = $[i]$, where i = 0,1 
		\end{center}
		\begin{center}
			$[1 \ 1 \ 1]^{T}$ $[0]$ = $[0 \ 0 \ 0]^{T}$
		\end{center}
		
		A linear code encoding k-bit messages into an m-bit codespace \textbf{C} is specified by an m x k generator matrix G whose entries are all elements of $\mathbb{Z}_2$. A code defined in this way is an [m, k] code. \newline 
		
		The generator matrix is used to encode information before sending it through the channel above but we have also said that a realistic channel is noisy so it can modify the codewords we send. Thus, we also need to define an operator to specify how an error occurs and to detect it. We will call such operators $\textbf{N}_j$ $\in \textbf{N}$ where $j$ denotes the element that will be modified in the original codeword. 
		Overall, the codeword is mapped by $\textbf{N}_j$ into codeword $c' =c + n_j$ where $\textbf{n}_j$ is an mx1 vector having a 1 at the jth row and zeros elsewhere, in such way we model the noise through sum modulo 2. \newline
		
		Having defined all of the above we can come to parity-check performed after transmission. Again we define a parity check matrix $\textbf{H}$ having size m-k x n.
		To check parity we can simply perform multiplication of H by the received codeword, the result will be: 
		
		\begin{center}
			\textbf{H}c = 0  
		\end{center}
		
		for any valid codeword. Instead, for any corrupted codeword the computation yields:
		
		\begin{center}
			$\textbf{H}c'$ = $\textbf{H} ( c + n_j )$ = $0 + \textbf{H} n_j $  = $\textbf{H}n_j $ 
		\end{center}
		
		which is the error syndrome. \newline
		
	\end{minipage}
	
	
	
	\begin{minipage}{1 \textwidth}
		
		The error syndrome serves as a crucial indicator of errors within a coding and decoding system. In the absence of errors or when only a single error occurs, the error syndrome is represented as 0. This indicates a clean transmission. Conversely, in the case of a single error, the error syndrome is denoted as Hnj, providing information about the specific bit, j, within the codeword where the error has taken place. Consequently, the corrupted codeword can be successfully decoded by correcting the jth bit. \newline
		
		However, when two errors, ni and nj, are present, the error syndrome becomes H(ni + nj). This scenario introduces potential ambiguity, as an additional error, nq, could exist such that nq equals ni + nj. In such cases, determining which bit to correct becomes uncertain, as the error syndrome could be associated with the qth bit instead of the intended jth bit. This ambiguity arises due to the possibility of multiple error combinations leading to the same syndrome. \newline
		
		To mitigate this ambiguity, it is essential to verify that any additional error, nq, does not satisfy the condition nq = ni + nj. If there are no such instances where nq equals ni + nj, the decoding process can proceed with confidence. However, if such instances are detected, ambiguity arises, and corrective action becomes challenging. One possible approach is to reject the codeword, prompting a request for retransmission from the sender. Alternatively, refining the encoding/decoding scheme may be considered to address and minimize the potential for ambiguous error correction, ensuring the reliability of the overall communication system. \newline
		
		This approach aims to maintain data integrity and prevent the propagation of errors within the encoded information. \newline
		
	\end{minipage}
	
	
	
	\begin{minipage}{1 \textwidth}
		
		Though, because of the ambiguity that can arise in error correction it is essential to discuss the detectability and correctability of errors. \newline
		
		We have mentioned how a codeword can be rejected if corrupted and how a receiver can ask to resend information. In the context of safeguarding the code against a set of noise operators, the efficacy of the encoding/decoding scheme is evaluated on its ability to either maintain the integrity of information or detect errors caused by each noise operator. Therefore, we can assert that a code successfully detects a noise operator $\textbf{N}$ if, for every codeword c within the code, the outcome is either $\textbf{N}$c = c or $\textbf{N}$c is not a member of the code $\textbf{C}$.\newline
		
		\textbf{Theorem 3.1.} $\textbf{N}$ is detectable by a code if and only if for each $c_m \neq c_n$ in the code, $\textbf{N}c_n \neq c_m $.\newline
		
		
		Error correction involves an active process where the decoder not only identifies errors but also rectifies them. 
		In the context of a code $\textbf{C}$ and a set of error operators $\textbf{N}$, the primary goal is to establish the presence of a decoding procedure capable of correcting errors introduced by $\textbf{N}$. 
		Assuming the existence of distinct codewords $c_m$ and $c_n$ within the code, and for specific indices i and j in $\textbf{N}$, let $c_q = \textbf{N}_ic_m = \textbf{N}_jc_n$. 
		This expression indicates that applying the error operator Ni to the codeword cm and the error operator $\textbf{N}_j$ to the codeword $c_n$ results in the same state $c_q$. In other words, after these specific errors occur, both codewords end up in the identical state represented by $c_q$. \newline
		
		When an unidentified error in N occurs, resulting in the state cq, the challenge lies in determining whether the original codeword was $c_m$ or $c_n$. 
		The error is unidentified meaning that it occurs in the set of error operators $\textbf{N}$, but we don't know which specific error operator caused it.  Because the same state $c_q$ can be obtained from applying different error operators to distinct codewords, the challenge arises when trying to determine whether the original codeword was cm or $c_n$. In simpler terms, given the state $c_q$, we cannot conclusively say which codeword was initially sent due to the ambiguity in identifying the specific error that occurred. \newline 
		
		\textbf{Theorem 3.1.} $\textbf{N}$ is correctable if and only if for all $c_m \neq c_n$ in the code and for all i, j, it is true that $\textbf{N}_ic_m$ $\neq$ $\textbf{N}_jc_n$. \newline
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		In the context of linear codes, we gain insights into the two essential steps of error recovery: error detection and error correction. A receiver may possess the capability for error detection only, prompting it to request retransmission of codewords upon detecting errors. Conversely, a receiver can be designed to autonomously correct errors after detection, although the effectiveness of error correction varies with different decoding schemes. \newline
		
		In the realm of code designs, matrix representations play a pivotal role in encoding/decoding procedures, offering a compact representation. This parallels the use of operators in quantum mechanics, where operators intuitively serve as encoding and decoding operations in quantum error-correcting codes. \newline
		
		The process of error correction in codewords involves measuring the syndrome, a representation that exclusively captures information about errors. This concept is particularly advantageous in quantum error-correcting codes. Unlike measurements in classical mechanics that often collapse the measured target, measuring the syndrome in quantum mechanics preserves the information of the data while also providing guidance on how to correct the errors. \newline
		
	\end{minipage}






 
	
	\begin{minipage}{1 \textwidth}
		
	%5
	\textbf{2. Quantum Error}\newline
		
		Due to the no-cloning theorem, which states that we cannot replicate quantum information in a quantum system, one might be brought to think that quantum error correction does not exists. Though we shall demonstarte that this problem can be solved, despite the three main issues that arise in quantum systems: \newline
		
		1. No Cloning Limitation: The attempt to implement a repetition code in the quantum realm by duplicating a quantum state multiple times violates the no-cloning theorem. This theorem unequivocally forbids the exact replication of an unknown quantum state. Even if cloning were feasible, the measurement and comparison of the three quantum states output from the channel would still be an insurmountable challenge.\newline
		
		2. Continuous Error Spectrum: Quantum errors manifest as a continuous spectrum, introducing a formidable challenge for error correction on a single qubit. Effectively identifying and correcting these errors seems to demand infinite precision, implying a need for infinite resources. The continuous nature of errors complicates the task of detecting and rectifying errors within a quantum system.\newline
		
		3. Information Loss through Measurement: In contrast to classical error-correction methods where observation aids in determining the appropriate decoding strategy, quantum mechanics presents a different issue. The act of measuring a quantum state typically results in the loss of quantum information contained in that state. As a consequence, the recovery of the original quantum information becomes impossible after measurement, introducing an additional layer of complexity to quantum error correction. \newline
		
		
		We shall now discuss how it is still possible to overcome such obstacles.\newline
		
		
		In the context of quantum computation, classical bits are replaced by quantum bits or qubits, which are represented using a two-state quantum system. More precisely, a qubit lives in an Hilbert space of dimension 2. We can represent the Hilbert space of a system of n qubits by means of a tensor product of Hilbert spaces referring to single qubits. A common physical realization of qubits involves spin-$\frac{1}{2}$ particles, such as electrons. The quantum state space of a qubit is spanned by two orthogonal basis states, conventionally denoted as $ |0 \rangle$ and $ |1 \rangle$, corresponding to the classical binary states 0 and 1. Mathematically, any qubit state 
		$ |\psi \rangle$ can be expressed as a superposition of these basis states:
		
		
		\begin{center}
			$\alpha |0 \rangle$ + $\beta$ $|1 \rangle$ =  $[ \ \alpha \ \ \beta \ ]^T $
		\end{center}
		
		where $\alpha$ and $\beta$ are complex numbers and satisfy the normalization condition $|\alpha|^2$ + $|\beta|^2$ = 1. This linear combination allows qubits to exist in a coherent superposition of both $|0 \rangle$ and $ |1 \rangle$  states simultaneously.  \newline
		When it comes to qubits, computations are performed through operators, which are the quantum logical gates represented as invertible square matrices. \newline
		
		%image
		
		
	\end{minipage}
 
	
	\begin{minipage}{1 \textwidth}
		
		Having briefly recalled the quantum setting, we shall now give the quantum version of the classical error model previously defined when talking about linear codes.
		Similarly to what we did with repetition codes, we can define the bit-flip error (or noise) for qubits throughthe $X$ gate apllied to $|\psi \rangle$. The state psi will be flipped wuth probability $p$.\newline
		
		We can suppose to introduce redundancy, firstly by mapping the physical basis of qubits to their respective logical counterparts in the following way:
		
		\begin{center}
			$\alpha |0 \rangle$ + $\beta$ $|1 \rangle$ $\rightarrow$ $\alpha |000 \rangle$ + $\beta$ $|111 \rangle$
		\end{center}
		
		where we have encoded the basis as:
		
		\begin{center}
			$|0 \rangle$ $\rightarrow$ $|0 \rangle_L$ $\equiv$ $|000 \rangle$   \\
		    $|1 \rangle$ $\rightarrow$ $|1 \rangle_L$ $\equiv$ $|111 \rangle$
		\end{center}
		
		where L stands for logical qubits, not physical. \newline
		If we imagine to pass all of the three bits one at a time through the channel and that one or fewer qubits are flipped, we can define a simple quantum error correction through detection and analysis of error syndromes through Projector operators, as we did in the classical case:\newline
		
		- $P_0$ = $|000 \rangle \langle 000|$ + $|111 \rangle \langle 111|$ no errors;\newline
		- $P_1$= $|100 \rangle \langle 100|$ + $|011 \rangle \langle 011|$ error 1st qubit;\newline
		- $P_2$ = $|010 \rangle \langle 010|$ + $|101 \rangle \langle 101|$ error 2nd qubit;\newline
		- $P_3$ = $|001 \rangle \langle 001|$ + $|110 \rangle \langle 110|$ error 3rd qubit;\newline
		
		In order tod detect the syndrome we will compute $\langle \psi| P_i | \psi \rangle $, which will be equal to 1 according to which bit i was flipped.
		Measuring the syndrome doesn't affect the state in any way, neither befor nor after the measurement. Furthermore, the syndrome gives no infromation of the state phase. We can recover the initial state with perfect accuracy by flipping again the qubit of interest.\newline
		Though, we have already mentioned that this schema works well only if errors occur on one or less of the three qubits.\newline
		
		While the error analysis is effective for discrete errors, it is acknowledged that not all errors and states in quantum mechanics are equal, due to the continuous nature of quantum states. To address this, the fidelity quantity is introduced. \newline
		The fidelity, denoted as $F(|\psi \rangle, \rho)$, is a measure of the overlap or similarity between a pure state $| \psi \rangle$  and a mixed state $\rho$. \newline
		
		The fidelity quantifies how well a quantum state is preserved or reconstructed after undergoing a process, such as error correction. 
		In the given scenario, where an error syndrome is measured and recovery procedures are applied, the fidelity is employed to assess how closely the corrected state aligns with the original, error-free state. 
		The fidelity helps evaluate the quality of the quantum error correction by measuring the similarity between the corrected state and the ideal state. \newline
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		The fidelity is particularly valuable when dealing with errors that may perturb or alter the quantum state in various ways. By maximizing the fidelity, quantum error correction aims to ensure that the corrected state closely approximates the original state, even in the presence of errors. It provides a quantitative measure of the accuracy and effectiveness of the error-correction process. \newline
		
	\end{minipage}







 
	\begin{minipage}{1 \textwidth}
		
		%5
		\textbf{2. Quantum Error}\newline
		
		Due to the no-cloning theorem, which states that we cannot replicate quantum information in a quantum system, one might be brought to think that quantum error correction does not exists. Though we shall demonstarte that this problem can be solved, despite the three main issues that arise in quantum systems: \newline
		
		1. No Cloning Limitation: The attempt to implement a repetition code in the quantum realm by duplicating a quantum state multiple times violates the no-cloning theorem. This theorem unequivocally forbids the exact replication of an unknown quantum state. Even if cloning were feasible, the measurement and comparison of the three quantum states output from the channel would still be an insurmountable challenge.\newline
		
		2. Continuous Error Spectrum: Quantum errors manifest as a continuous spectrum, introducing a formidable challenge for error correction on a single qubit. Effectively identifying and correcting these errors seems to demand infinite precision, implying a need for infinite resources. The continuous nature of errors complicates the task of detecting and rectifying errors within a quantum system.\newline
		
		3. Information Loss through Measurement: In contrast to classical error-correction methods where observation aids in determining the appropriate decoding strategy, quantum mechanics presents a different issue. The act of measuring a quantum state typically results in the loss of quantum information contained in that state. As a consequence, the recovery of the original quantum information becomes impossible after measurement, introducing an additional layer of complexity to quantum error correction. \newline
		
		
		We shall now discuss how it is still possible to overcome such obstacles.\newline
		
		
		In the context of quantum computation, classical bits are replaced by quantum bits or qubits, which are represented using a two-state quantum system. More precisely, a qubit lives in an Hilbert space of dimension 2. We can represent the Hilbert space of a system of n qubits by means of a tensor product of Hilbert spaces referring to single qubits. A common physical realization of qubits involves spin-$\frac{1}{2}$ particles, such as electrons. The quantum state space of a qubit is spanned by two orthogonal basis states, conventionally denoted as $ |0 \rangle$ and $ |1 \rangle$, corresponding to the classical binary states 0 and 1. Mathematically, any qubit state 
		$ |\psi \rangle$ can be expressed as a superposition of these basis states:
		
		
		\begin{center}
			$\alpha |0 \rangle$ + $\beta$ $|1 \rangle$ =  $[ \ \alpha \ \ \beta \ ]^T $
		\end{center}
		
		where $\alpha$ and $\beta$ are complex numbers and satisfy the normalization condition $|\alpha|^2$ + $|\beta|^2$ = 1. This linear combination allows qubits to exist in a coherent superposition of both $|0 \rangle$ and $ |1 \rangle$  states simultaneously.  \newline
		When it comes to qubits, computations are performed through operators, which are the quantum logical gates represented as invertible square matrices. \newline
		
		%image
		
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		Having briefly recalled the quantum setting, we shall now give the quantum version of the classical error model previously defined when talking about linear codes.
		Similarly to what we did with repetition codes, we can define the bit-flip error (or noise) for qubits throughthe $X$ gate apllied to $|\psi \rangle$. The state psi will be flipped wuth probability $p$.\newline
		
		We can suppose to introduce redundancy, firstly by mapping the physical basis of qubits to their respective logical counterparts in the following way:
		
		\begin{center}
			$\alpha |0 \rangle$ + $\beta$ $|1 \rangle$ $\rightarrow$ $\alpha |000 \rangle$ + $\beta$ $|111 \rangle$
		\end{center}
		
		where we have encoded the basis as:
		
		\begin{center}
			$|0 \rangle$ $\rightarrow$ $|0 \rangle_L$ $\equiv$ $|000 \rangle$   \\
		    $|1 \rangle$ $\rightarrow$ $|1 \rangle_L$ $\equiv$ $|111 \rangle$
		\end{center}
		
		where L stands for logical qubits, not physical. \newline
		If we imagine to pass all of the three bits one at a time through the channel and that one or fewer qubits are flipped, we can define a simple quantum error correction through detection and analysis of error syndromes through Projector operators, as we did in the classical case:\newline
		
		- $P_0$ = $|000 \rangle \langle 000|$ + $|111 \rangle \langle 111|$ no errors;\newline
		- $P_1$= $|100 \rangle \langle 100|$ + $|011 \rangle \langle 011|$ error 1st qubit;\newline
		- $P_2$ = $|010 \rangle \langle 010|$ + $|101 \rangle \langle 101|$ error 2nd qubit;\newline
		- $P_3$ = $|001 \rangle \langle 001|$ + $|110 \rangle \langle 110|$ error 3rd qubit;\newline
		
		In order tod detect the syndrome we will compute $\langle \psi| P_i | \psi \rangle $, which will be equal to 1 according to which bit i was flipped.
		Measuring the syndrome doesn't affect the state in any way, neither befor nor after the measurement. Furthermore, the syndrome gives no infromation of the state phase. We can recover the initial state with perfect accuracy by flipping again the qubit of interest.\newline
		Though, we have already mentioned that this schema works well only if errors occur on one or less of the three qubits.\newline
		
		While the error analysis is effective for discrete errors, it is acknowledged that not all errors and states in quantum mechanics are equal, due to the continuous nature of quantum states. To address this, the fidelity quantity is introduced. \newline
		The fidelity, denoted as $F(|\psi \rangle, \rho)$, is a measure of the overlap or similarity between a pure state $| \psi \rangle$  and a mixed state $\rho$. \newline
		
		The fidelity quantifies how well a quantum state is preserved or reconstructed after undergoing a process, such as error correction. 
		In the given scenario, where an error syndrome is measured and recovery procedures are applied, the fidelity is employed to assess how closely the corrected state aligns with the original, error-free state. 
		The fidelity helps evaluate the quality of the quantum error correction by measuring the similarity between the corrected state and the ideal state. \newline
		
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		The fidelity is particularly valuable when dealing with errors that may perturb or alter the quantum state in various ways. By maximizing the fidelity, quantum error correction aims to ensure that the corrected state closely approximates the original state, even in the presence of errors. It provides a quantitative measure of the accuracy and effectiveness of the error-correction process. \newline
		
		We can formally define the Quantum Error Model as we did in Classical computing with the Binary Symmetric Channel. Take quantum information to be a $|\psi \rangle$ in an Hilbert space $\textit{H}_A$. We can introduce ancillary qubits as to introduce redundancy. Ancillary qubits are replicas of information that will be manipulated along with the data that we want to transmit.
		Ancillary bits are part of an Hilbert space that we will call $\textit{H}_B$, they are initially in a state $|b \rangle$. When we encode information what we do is to map the tensor product of $|\psi \rangle$ and $|b \rangle$ in a new Hilbert space $\textit{H}_C$ (the code space) through the application of an encoder operator $\textbf{E}$, which should be the equivalent of the generator matrix in classic models. In the same way we can define an Hilbert space to model the noise, we will call it $\textit{H}_N$; this space more generally represents the noisy environment that introduces errors. The environment is initially in state $|n \rangle$. The interaction between information and the environment is computed as:\newline
		
		\begin{center}
			$|c' \rangle$ = $|c \rangle$ $\otimes$ $|n \rangle$ = $\sum_{i} (N_i c) \otimes |n_i \rangle $
		\end{center}
		
	    with $N_i$ being a noise operator acting on i.
	    When we decode information through the decoding operator $\textbf{D}$ we want to map $|c' \rangle$ back to $|\psi \rangle$ $\otimes$ $|s_i \rangle$, identifying teh original state psi and the error syndrome. We can then use the error syndrome to correct psi and get the initial state. To actually do exactly what we did in the clasical case, the error syndrome should be homomorfic to $N_i$ to recover the respective errors, then s should be also independent from every psi, otehrwise we would not be able to retrieve the message and make adjustments. Note that all $N_i$ cane be made linearly independent to avoid confounding error. Note also that $\sum_{i} N_i^{\dagger} N_i= I$ ensures that the $Tr[\sum_{i} N_i^{\dagger} \rho N_i]= 1$ with rho a general quantum state in  $\textbf{C}$. \newline
	    
	    The above description of quantum error model actually contains the conditions on error correctability to protect quantum information. 
	    This conditions are summed up into the equation given by the following theorem: \newline
	    
	    \textbf{Theorem 2.1.} Given a quantum code $C$ ($H_C$) and $P$ the projector onto $C$. Suppose $N$ is a quantum operation with operation elements $\{N_i\}$. A necessary and sufficient condition for the existence of an error-correction operation $R$ correcting $N$ on $C$ is that
	   
	    \begin{center}
	    	$PN_i^{\dagger}N_jP = \lambda_{ij}P$
	    \end{center}
	    
	    for some Hermitian matrix $\lambda$ of complex numbers. For all $N_i$. \newline
	    
	    The projector is a mathematical operation that projects any quantum state onto the code space $C$. It ensures that any state acted upon by the error correction process is effectively confined to the subspace defined by the error-correcting code $C$ since, when quantum states undergo errors, these errors might cause the state to deviate from the code space. 
	    
	    
	    

		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
		The condition between the error correction operation R and the error operation N within the code space C essentially states that the error correction is successful when applied to the states within C, ensuring that the corrected state remains within the code space.
		
		We could say that it essentially ensure that a noisy quantum state is not too noisy. P forces the correction operation to act within the space where it provides reliable error-correction. In this sense P protects against (excessive) noise.
		
		
		The $N$s actually represent noise operators and if such an operation exists we say that $\{N_i\}$ constitutes a correctable set of errors. This would make $C$ correctable. \newline
		
		 %
		Verifying Theorem 2.1 can be difficult so we describe a theoretical formalism which uses the quantum error-correction conditions as a starting point for the construction of interesting classes of codes: the Stabilizer formalism.
		
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
	\end{minipage}
	
	
	
	
	
	\begin{minipage}{1 \textwidth}
	
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
	\end{minipage}
	
	\begin{minipage}{1 \textwidth}
		
	\end{minipage}
	
	
	
	\begin{minipage}{1 \textwidth}
 
		\textbf{3. Fault-tolerant compuation}\newline 
		
		\textbf{4. Gates and Computations in the Toric Code}\newline 
		
		%20 pagine
		%https://viterbi-web.usc.edu/~tbrun/Course/
		%https://arxiv.org/pdf/quant-ph/0602157.pdf
		
		
	\end{minipage}
	











 
	
	\begin{thebibliography}{99} % "99" è solo un esempio del numero massimo di voci che ti aspetti nella tua bibliografia
		\bibitem{ref1} A. Kitaev, Ann. Phys. (N. Y). 303, 2 (2003),
		arXiv:9707021 [quant-ph].
		
		%\bibitem{ref2}D. Arovas, J. R. Schrieffer, and F. Wilczek,
		%Phys. Rev. Lett. 53, 722 (1984).
		
		%\bibitem{ref3}H. Bartolomei, M. Kumar, R. Bisognin,
		%A. Marguerite, J.-M. Berroir, E. Bocquillon, B. Pla¸cais, A. Cavanna, Q. Dong,
		%U. Gennser, Y. Jin, and G. F`eve, Science
		%(80-. ). 368, 173 (2020), arXiv:2006.13157.
		
		\bibitem{ref4} D. Browne, (2014). "Topological Codes and Computation,".
		
		\bibitem{ref5} S. B. Bravyi and A. Y. Kitaev, (1998).
		arXiv:9811052 [quant-ph].
		
		\bibitem{ref6}  Rao, S., (2017). Introduction to abelian and non-abelian anyons. In: Bhattacharjee, S., Mj, M., Bandyopadhyay, A. (eds) Topology and Condensed Matter Physics. Texts and Readings in Physical Sciences, vol 19. Springer, Singapore. %https://doi.org/10.1007/978-981-10-6841-6_16
		
		%\bibitem{ref6}R. Raussendorf, J. Harrington, and
		%K. Goyal, New J. Phys. 9, 199 (2007),
		%arXiv:0703143 [quant-ph].
		
		\bibitem{ref7} L. Oddis, Ph.D. Thesis in Mathematics: "Two-Anyon Schrodinger Operators".
		
		\bibitem{ref} A. Hatcher, Algebraic Topology, Cambridge University Press, 2002.
		
		\bibitem{ref8} Nielsen, M. A., Chuang, I. L. (2011). Quantum Computation and Quantum Information: 10th Anniversary Edition. Cambridge University Press.
		
		\bibitem{ref9} Preskill, J. (1998). Fault-tolerant quantum computation. An Introduction to quantum computation and information (pp. 213-269).
		
		\bibitem{ref} A. Pondini, (2020) Quantum error correction e toric code.
		
		\bibitem{ref10} Daley, A.J., Bloch, I., Kokail, C. et al. Practical quantum advantage in quantum simulation.
		
		\bibitem{ref11} Herman, D., Googin, C., Liu, X. et al. Quantum computing for finance. Nat Rev Phys 5, 450–465 (2023).
		
		\bibitem{ref12} V.E. Elfving, B.W. Broer, M. Webber, J. Gavartin, M.D. Halls, K. P. Lorton, A. Bochevarov, arXiv:2009.12472 [quant-ph].
		
		\bibitem{ref12} S. Lang, Linear Algebra, Springer (pp. 191-205).
		
		\bibitem{ref12} Wilczek, F. (1991). Anyons. Scientific American, 264(5), 58–65. http://www.jstor.org/stable/24936902


                \bibitem{ref12}	arXiv:quant-ph/0602157
  
	\end{thebibliography}
	

\end{document}
